---
title: "SL_Pipeline"
format: html
editor: visual
---

## Set environment

```{r}
#| warning: false
require(dplyr)
require(janitor)
require(fastDummies)
require(missMethods)
require(tidymodels)
require(recipeselectors)
require(parsnip)
require(yardstick)
require(vip)
require(ggplot2)
require(treeshap)
require(shapviz)
require(kernelshap)
require(dcurves)
require(knitr)
require(tableone)
require(SuperLearner)
require(caret)
require(MLmetrics)
require(splitstackshape)
require(RhpcBLASctl)
require(xgboost)
require(betacal)
require(mlbench)
require(givitiR)
require(ggpubr)
```

## Set pipeline parameters

```{r}
outcome <- 'madrs_response' # set to: madrs_response or madrs_remission
n_folds <- 10
filter_features <- TRUE
missingness_threshold <- 0.3
c_cutoff <- 0.9
cv_shuff <- FALSE # set to true later

# set paths
path_data <- '/Users/ls1002/Doucments/Coding/ENGINE_R/data/'
path_code <- '/Users/ls1002/Doucments/Coding/ENGINE_R/'
```

## Format data

```{r}
# load data
data <- read.csv(paste0(path_data, 'responder_status.csv'))

# dummy variable encoding
data <- fastDummies::dummy_cols(data, select_columns = c(grep("protocol", names(data), value = T), "bobine", "dx_principal"), remove_selected_columns=TRUE, ignore_na=TRUE)

# temporary handling of episode_passe_bl: consider bins later
data$episode_passe_bl <- as.numeric(gsub('>=', '', data$episode_passe_bl))

# clean up names and drop unwanted columns
data <- data %>%
  janitor::clean_names() %>%
  select(-x)

# calculate MADRS total scores
data$madrs_total_bl <- data %>% 
  select(matches("madrs_[0-9]+_bl")) %>%
  rowSums(na.rm=FALSE)

data$madrs_total_fu <- data %>% 
  select(matches("madrs_[0-9]+_fu")) %>%
  rowSums(na.rm=FALSE)

# remove observations with missing baseline or follow-up scores
data <- data %>%
  filter(!is.na(madrs_total_bl)) %>%
  filter(!is.na(madrs_total_fu))

## binarize outcomes
# response
data$madrs_response <- ifelse( ((data$madrs_total_fu - data$madrs_total_bl) / data$madrs_total_bl) <= -0.5, 1, 0)

# remission
data$madrs_remission <- ifelse(data$madrs_total_fu <= 10, 1, 0)

# set outcome 
y <- data[[outcome]]

# drop follow-up variables
data_ss <- data[, !names(data) %in% c(grep('_fu|response|remission', names(data), value = T), 'id')]

# reset outcome
data_clf <- data.frame(
  outcome=y,
  data_ss
)

# drop columns with high missingness
percent_missing_cols <- apply(data_clf, 2, function(c){
  sum(is.na(c)) / length(c)
})

data_clf <- data_clf[, !names(data_clf) %in% names(which(percent_missing_cols >= missingness_threshold))]

print(sprintf('Dropping high missingness columns: %s', names(which(percent_missing_cols >= missingness_threshold))))

## Median impute: temporary handling
data_clf <- data_clf %>%
  mutate(across(
    where(is.numeric),
    ~replace_na(., if (is.integer(.)) as.integer(median(., na.rm = TRUE)) else median(., na.rm = TRUE))
  ))
```

## Patient table (before imputation)

```{r}
patient_summary_table <- function(data, outcome){
  
  # summarize
  cat_vars <- grep('sexe|cigarette_bl|alcool_bl|cocaine_bl|
  revenu_bl|episode_passe_bl|psychotherapie_bl|ect_bl|rtms_bl|tdcs_bl|
  hospitalisation_bl|tx_protocol_|bobine_|
  madrs_response|madrs_remission', names(data), value = T)
  
  summary_vars <- unique(c(grep('age|age_onset_bl|annees_education_bl|
                                duree_episode_bl|madrs_total_', names(data), value = T), cat_vars))
  
  if(outcome=='madrs_remission'){
  
    tab_one <- CreateTableOne(vars=summary_vars,
                              strata = 'madrs_remission',
                              factorVars = cat_vars,
                              addOverall = TRUE,
                              data=data)
  }else{
    
    tab_one <- CreateTableOne(vars=summary_vars,
                              strata = 'madrs_response',
                              factorVars = cat_vars,
                              addOverall = TRUE,
                              data=data)
    
  }
  
  kableone(tab_one)
  
}

patient_summary_table(data = data, outcome = outcome)
```

## Set X, y and optionally filter predictors

```{r}
X <- data_clf %>%
  dplyr::select(-outcome)

y <- data_clf$outcome

if(filter_features){
  # Filter near-zero variance variables
  X <- X[, !names(X) %in% nzv(X, names = TRUE)]
  
  # filter highly correlated variables
  cm <- cor(X)
  X <- X[, !names(X) %in% findCorrelation(cm, names =  TRUE, cutoff = c_cutoff)]
}
```

## Set multicore processing

```{r}
(num_cores = RhpcBLASctl::get_num_cores())
options(mc.cores = num_cores - 2)
getOption("mc.cores")
```

## Add additional base learners for super learner model

```{r}
# ranger
mtry_seq <- c(2, floor(sqrt(ncol(X)) * c(0.5, 1)))
ranger_learners <- create.Learner("SL.ranger", tune = list(mtry = mtry_seq, num.trees=c(500, 1000)), detailed_names = TRUE)

# glmnet/elastic net
enet_learners <- create.Learner("SL.glmnet", tune = list(alpha = seq(0, 1, length.out=4)), detailed_names = TRUE)

xgb_tune <- list(ntrees = c(50, 100),
                max_depth = 1:3,
                shrinkage = c(0.001, 0.01, 0.1))
xgb_learners <- create.Learner("SL.xgboost", tune = xgb_tune, detailed_names = TRUE, name_prefix = "xgb")

svm_tune <- list(cost = c(0.1, 1, 10),
                 gamma = c(0.001, 0.01, 0.1, 1),
                 kernel = 'radial')

svm_learners <- create.Learner("SL.svm", tune = svm_tune, detailed_names = TRUE, name_prefix = 'svm') 
```

## Fit super learner model

```{r}
set.seed(1, "L'Ecuyer-CMRG")
cv_sl = CV.SuperLearner(Y = y, X = X, family = binomial(),
                        cvControl = list(V = n_folds, stratifyCV = TRUE, shuffle = cv_shuff),
                        parallel = "multicore",
                        method = "method.AUC",
                        SL.library = c("SL.mean",
                                       enet_learners$names, "SL.glmnet",
                                       ranger_learners$names, "SL.ranger",
                                       xgb_learners$names, "SL.xgboost",
                                       svm_learners$names, "SL.svm"))
```

## Model performance

### Summary: AUC

```{r}
summary(cv_sl)
```

### Weight distribution

```{r}
review_weights = function(cv_sl) {
  meta_weights = coef(cv_sl)
  means = colMeans(meta_weights)
  sds = apply(meta_weights, MARGIN = 2,  FUN = sd)
  mins = apply(meta_weights, MARGIN = 2, FUN = min)
  maxs = apply(meta_weights, MARGIN = 2, FUN = max)
  # Combine the stats into a single matrix.
  sl_stats = cbind("mean(weight)" = means, "sd" = sds, "min" = mins, "max" = maxs)
  # Sort by decreasing mean weight.
  sl_stats[order(sl_stats[, 1], decreasing = TRUE), ]
}

print(review_weights(cv_sl), digits = 3)
```

### Plot learner performance

```{r}
plot(cv_sl)
```

## Check calibration

```{r}
# save predicted probabilities for each model
sl_probs <- data.frame(cv_sl$library.predict)
sl_probs$SL_model <- cv_sl$SL.predict
sl_probs$outcome <- y

# Fit beta calibration model
calibration_model <- beta_calibration(
  p = sl_probs$SL_model,
  y = sl_probs$outcome,
  parameters = "ab" # options "abm", "ab", "am" corresponding to 3, 2, or 1 shape parameter
)

calibrated_probs <- beta_predict(sl_probs$SL_model, calibration_model)

# check calibration with giviti
belt_uncal <- givitiCalibrationBelt(sl_probs$outcome, sl_probs$SL_model, devel = "external")
belt_cal <- givitiCalibrationBelt(sl_probs$outcome, calibrated_probs, devel = "external")

par(mfrow=c(1, 2))
plot(belt_uncal, main = 'Uncalibrated')
plot(belt_cal, main = 'Beta Calibration')

if(belt_uncal$statistic <= belt_cal$statistic){
  print("Uncalibrated model GiViTI statistic value is as good or better than beta calibration")
  print(sprintf("Beta calibration = %s; Uncalibrated = %s", belt_cal$statistic, belt_uncal$statistic))
  
  # set final model
  sl_probs$SL_model_final <- sl_probs$SL_model
  
}else{
  print("Beta calibrated model GiViTI statistic value is better than uncalibrated")
  print(sprintf("Beta calibration = %s; Uncalibrated = %s", belt_cal$statistic, belt_uncal$statistic))
  
  # set final model
  sl_probs$SL_model_final <- calibrated_probs
}
```

### ROC and PR Curves

```{r}

# identify factor levels to determine event_level = first or second
print(levels(as.factor(sl_probs$outcome))) # is "1" the first or second level?

roc_plot <- sl_probs %>%
  mutate(outcome=as.factor(outcome)) %>%
  roc_curve(outcome, SL_model_final, event_level = 'second') %>%
  autoplot()

pr_plot <- sl_probs %>%
  mutate(outcome=as.factor(outcome)) %>%
  pr_curve(outcome, SL_model_final, event_level = 'second') %>%
  autoplot()

ggarrange(roc_plot, pr_plot, ncol = 2, nrow = 1)

# AUC-ROC value
sl_probs %>%
  mutate(outcome=as.factor(outcome)) %>%
  yardstick::roc_auc(outcome, SL_model_final, event_level = 'second')

# AUC-PR value
sl_probs %>%
  mutate(outcome=as.factor(outcome)) %>%
  yardstick::pr_auc(outcome, SL_model_final, event_level = 'second')
```

## Decision Curve Analysis

```{r}
dc_data <- dcurves::dca(outcome ~ SL_model_final, data = sl_probs)
dc_data

# calculate difference in model-based versus treat-all-based net benefit
dc_df <- data.frame(dc_data$dca)
dc_df_ta <- dc_df[dc_df$label=='Treat All', ]
dc_df_mod <- dc_df[dc_df$label=='SL_model_final', ]
dc_df_mod$net_benefit_gain <- dc_df_mod$net_benefit - dc_df_ta$net_benefit

# subset over "reasonable range" and region of imporovement
tmp <- dc_df_mod[dc_df_mod$threshold < 0.45 & dc_df_mod$net_benefit_gain > 0, ]
tmp$nnt_gain <- (1/tmp$net_benefit_gain)
tmp$log_nnt_gain <- log(tmp$nnt_gain)


p1 <- ggplot(tmp, aes(threshold, net_benefit_gain)) + 
  geom_point(col='lightblue', size = 2) + 
  geom_line() + 
  ylab("Net Benefit Gain") + 
  xlab("Threshold Probability") + 
  theme_bw()

p2 <- ggplot(tmp, aes(threshold, log_nnt_gain)) + 
  geom_point(col='lightblue', size = 2) + 
  geom_line() + 
  ylab("Log NNT Gain") + 
  xlab("Threshold Probability") + 
  theme_bw()

ggarrange(p1, p2, ncol=2, nrow=1)

# report gain in NNT quantiles
kable(quantile(tmp$nnt_gain), caption = "NNT Gain Quantiles")
```

## Remission rates by cross-validated remission probability deciles

```{r}
pdist <- sl_probs$SL_model_final

decile_membership <- ntile(pdist, 9)

decile_df <- data.frame(
  pdist=pdist,
  remission=y,
  decile=decile_membership
)

decile_df %>%
  group_by(decile) %>%
  summarise(support = n(),
            min_p = min(pdist),
            proportion_remission = sum(remission==1) / n()) %>%
  kable()

decile_plt <- decile_df %>%
  group_by(decile) %>%
  summarise(support = n(),
            min_p = min(pdist),
            proportion_remission = sum(remission==1) / n()) 
  
ggplot(decile_plt, aes(as.factor(decile), proportion_remission)) + 
  geom_bar(stat = 'identity') + 
  ylim(0, 1) + 
  geom_hline(yintercept=0.5, linetype="dashed", color = "red") +
  theme_bw()
```

## SHAP analysis

```{r}
refit_data <- data.frame(outcome=y, X)
refit_data$outcome <- as.factor(refit_data$outcome)

# refit best learner model: Change this model based on SL learner performance but xgb can be a placeholder
xgb_mod <- boost_tree(mode = 'classification', 
                      trees = 50,
                      learn_rate = 0.1,
                      tree_depth = 3) %>%
  set_engine('xgboost')

rec <- 
  recipe(outcome ~ ., data = refit_data)

# set workflow
xgb_workflow <- 
  workflow() %>% 
  add_recipe(rec) %>% 
  add_model(xgb_mod)

shap_mod <- xgb_workflow %>%
  fit(refit_data)

shap_data <- refit_data %>% dplyr::select(-outcome)

pred_fun <- function(object, newdata) {
  predict(object, newdata, type = "prob") %>% dplyr::select(2)  # Probability of class "1"
}

set.seed(1)
shap_sample_index <- sample(x=1:nrow(shap_data), size = 50, replace = FALSE)

shap_explainer <- kernelshap(shap_mod, 
                             X = shap_data, 
                             bg_X = shap_data[shap_sample_index, ],
                             pred_fun = pred_fun, 
                             verbose = FALSE)
```

### Beeswarm plot

```{r}
sv <- shapviz(shap_explainer, X = shap_data)
beeswarm <- sv_importance(sv, kind = "bee")
beeswarm
```
